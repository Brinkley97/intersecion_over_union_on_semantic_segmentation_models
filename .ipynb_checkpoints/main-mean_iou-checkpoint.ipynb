{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bbd9c8ac-92a0-4d65-88f5-8b9423c5a053",
   "metadata": {},
   "source": [
    "# Mean IOU for Semantic Segmetation Model\n",
    "- [Brinkley97/PixelLib model GitHub Repo](https://github.com/Brinkley97/sem_seg_pxLib_ade20k/blob/main/ssForImg.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2f3cbb70-8096-4578-8227-b170d62e72e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00b34605-6372-4e93-b835-d76f833db63e",
   "metadata": {},
   "source": [
    "## Load Semantic Segmetation Images\n",
    "- Update method to load multiple images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e1a6e5f7-523e-457a-85ca-c53716e17583",
   "metadata": {},
   "outputs": [],
   "source": [
    "base = \"/Users/brinkley97/Documents/development/catalyst_robotics/iou/\"\n",
    "ground_truth_path = base + \"ground_truth_rural_input_images/\"\n",
    "ground_truth_directory = os.listdir(\"ground_truth_rural_input_images/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "9a0869be-167e-4984-bd60-52170a165938",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_miou(ground_truth_image, prediction_model_image):\n",
    "    '''Determine the intersection over union when given a ground truth image and prediction model image\n",
    "    \n",
    "    Arguments:\n",
    "    ground_truth_image -- string\n",
    "    prediction_model_image -- string\n",
    "    \n",
    "    Return:\n",
    "    iou\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    # print(ground_truth_image)\n",
    "    # print(prediction_model_image)\n",
    "    \n",
    "    '''\n",
    "    Pre process ground_truth image for MeanIOU\n",
    "    Get RGB to pass into TF MeanIOU function\n",
    "    '''\n",
    "    open_ground_truth_image = Image.open(ground_truth_image)\n",
    "    ground_truth_rgb = np.asarray(open_ground_truth_image)\n",
    "    print(\"****ground_truth_rgb size:\", np.shape(ground_truth_rgb))\n",
    "#    print(\"ground_truth_rgb image:\", ground_truth_rgb)\n",
    "    \n",
    "    '''\n",
    "    Pre process prediction_model image for MeanIOU\n",
    "    Get RGB to pass into TF MeanIOU function\n",
    "    '''\n",
    "    open_prediction_model_image = Image.open(prediction_model_image)\n",
    "    prediction_model_rgb = np.asarray(open_prediction_model_image)\n",
    "    # print(\"****pre-reshape prediction_model_rgb size:\", np.shape(prediction_model_rgb))\n",
    "#    print(\"prediction_model_rgb image:\", prediction_model_rgb)\n",
    "    \n",
    "    if np.shape(ground_truth_rgb) != np.shape(prediction_model_rgb):\n",
    "        # print(True)\n",
    "        resized_prediction_model_rgb = np.resize(prediction_model_rgb, np.shape(ground_truth_rgb))\n",
    "        # print(\"resized_prediction_model_rgb size:\", np.shape(resized_prediction_model_rgb))\n",
    "    else:\n",
    "        resized_prediction_model_rgb = prediction_model_rgb\n",
    "    \n",
    "    '''\n",
    "    Compute the mean intersection over union with TF\n",
    "    https://www.tensorflow.org/api_docs/python/tf/keras/metrics/MeanIoU\n",
    "    update_state method: https://github.com/keras-team/keras/blob/v2.10.0/keras/metrics/metrics.py#L2686-L2736\n",
    "    '''\n",
    "    \n",
    "    mean_iou = tf.keras.metrics.MeanIoU(num_classes=25)\n",
    "    print(mean_iou)\n",
    "    # print(\"prediction_model_rgb size:\", np.shape(prediction_model_rgb), prediction_model_rgb)\n",
    "    # print(\"resized_prediction_model_rgb size:\", np.shape(resized_prediction_model_rgb), resized_prediction_model_rgb)\n",
    "    mean_iou.update_state(ground_truth_rgb, resized_prediction_model_rgb)\n",
    "    \n",
    "    return mean_iou.result().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "0b2d16d0-503a-4eed-9e8e-a26e6c41bf0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_two_images(stored_images):\n",
    "    '''Pass stored images into the calc_miou function\n",
    "    \n",
    "    Arguments:\n",
    "    stored_images -- dict\n",
    "    \n",
    "    Functions:\n",
    "    calc_miou\n",
    "    \n",
    "    Return:\n",
    "    iou for each set of images -- list\n",
    "    '''\n",
    "\n",
    "    store_iou = []\n",
    "    for key, value in stored_images.items():\n",
    "        ground_truth_image = key\n",
    "        print(\"The ground_truth_image is :\", ground_truth_image)\n",
    "        \n",
    "        prediction_model_image = value\n",
    "        print(\"The prediction_model_image is : \", prediction_model_image)\n",
    "        \n",
    "        i_o_u = calc_miou(ground_truth_image, prediction_model_image)\n",
    "        print(\"The Mean IOU for these two images : \", i_o_u, \"\\n\")\n",
    "        store_iou.append(i_o_u)\n",
    "        \n",
    "    return store_iou"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "f099171a-5eee-471b-8539-5b757cb1b996",
   "metadata": {},
   "outputs": [],
   "source": [
    "def models_to_use(base, ground_truth_path, prediction_model_path, ground_truth_directory, prediction_model_directory):  \n",
    "    '''Set up path to get images\n",
    "    \n",
    "    Arguments:\n",
    "    base -- string\n",
    "    ground_truth_path -- string\n",
    "    prediction_model_path -- string\n",
    "    ground_truth_directory -- list\n",
    "    prediction_model_directory -- list\n",
    "    \n",
    "    Functions:\n",
    "    calc_iou\n",
    "    \n",
    "    Return:\n",
    "    iou for each set of images\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    ground_truth_path += '{0}'\n",
    "    prediction_model_path += '{0}'\n",
    "    \n",
    "    ground_truth_images = [ground_truth_path.format(idx) for idx in ground_truth_directory]\n",
    "    prediction_model_images = [prediction_model_path.format(idx) for idx in prediction_model_directory]\n",
    "    # print(ground_truth_images, '\\n', prediction_model_images)\n",
    "    \n",
    "#    Store ground_truth_images as key and m_seg_images as value\n",
    "    store_images = {}\n",
    "    \n",
    "    \n",
    "    for idx in range(len(ground_truth_images)):\n",
    "        store_images.update({ground_truth_images[idx]: prediction_model_images[idx]})\n",
    "    \n",
    "    miou_per_set_of_images = compare_two_images(store_images)\n",
    "    return miou_per_set_of_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "742be78b-af63-4558-80c4-0131c810483e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The ground_truth_image is : /Users/brinkley97/Documents/development/catalyst_robotics/iou/ground_truth_rural_input_images/trail-6_00001.png\n",
      "The prediction_model_image is :  /Users/brinkley97/Documents/development/catalyst_robotics/iou/m_seg_rural_input_images/trail-6_00001.png\n",
      "****ground_truth_rgb size: (550, 688, 3)\n",
      "MeanIoU(num_classes=25,name=mean_io_u_8,dtype=float32)\n",
      "prediction_model_rgb size: (449, 558, 4) [[[255 255 255   0]\n",
      "  [255 255 255   0]\n",
      "  [255 255 255   0]\n",
      "  ...\n",
      "  [255 255 255   0]\n",
      "  [255 255 255   0]\n",
      "  [255 255 255   0]]\n",
      "\n",
      " [[255 255 255   0]\n",
      "  [255 255 255   0]\n",
      "  [255 255 255   0]\n",
      "  ...\n",
      "  [255 255 255   0]\n",
      "  [255 255 255   0]\n",
      "  [255 255 255   0]]\n",
      "\n",
      " [[255 255 255   0]\n",
      "  [255 255 255   0]\n",
      "  [255 255 255   0]\n",
      "  ...\n",
      "  [255 255 255   0]\n",
      "  [255 255 255   0]\n",
      "  [255 255 255   0]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[255 255 255   0]\n",
      "  [255 255 255   0]\n",
      "  [255 255 255   0]\n",
      "  ...\n",
      "  [255 255 255   0]\n",
      "  [255 255 255   0]\n",
      "  [255 255 255   0]]\n",
      "\n",
      " [[255 255 255   0]\n",
      "  [255 255 255   0]\n",
      "  [255 255 255   0]\n",
      "  ...\n",
      "  [255 255 255   0]\n",
      "  [255 255 255   0]\n",
      "  [255 255 255   0]]\n",
      "\n",
      " [[255 255 255   0]\n",
      "  [255 255 255   0]\n",
      "  [255 255 255   0]\n",
      "  ...\n",
      "  [255 255 255   0]\n",
      "  [255 255 255   0]\n",
      "  [255 255 255   0]]]\n",
      "resized_prediction_model_rgb size: (550, 688, 3) [[[255 255 255]\n",
      "  [  0 255 255]\n",
      "  [255   0 255]\n",
      "  ...\n",
      "  [  0 255 255]\n",
      "  [255   0 255]\n",
      "  [255 255   0]]\n",
      "\n",
      " [[255 255 255]\n",
      "  [  0 255 255]\n",
      "  [255   0 255]\n",
      "  ...\n",
      "  [  0 255 255]\n",
      "  [255   0 255]\n",
      "  [255 255   0]]\n",
      "\n",
      " [[255 255 255]\n",
      "  [  0 255 255]\n",
      "  [255   0 255]\n",
      "  ...\n",
      "  [  0 255 255]\n",
      "  [255   0 255]\n",
      "  [255 255   0]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[113 241 188]\n",
      "  [255  91 219]\n",
      "  [170 255  79]\n",
      "  ...\n",
      "  [255  65 191]\n",
      "  [119 255  52]\n",
      "  [181 111 255]]\n",
      "\n",
      " [[ 39 169  98]\n",
      "  [255  37 167]\n",
      "  [ 97 255  36]\n",
      "  ...\n",
      "  [255  77 213]\n",
      "  [163 255  96]\n",
      "  [227 173 255]]\n",
      "\n",
      " [[119 247 189]\n",
      "  [255 126 253]\n",
      "  [192 255 125]\n",
      "  ...\n",
      "  [255  67 203]\n",
      "  [152 255  59]\n",
      "  [197 146 255]]]\n"
     ]
    },
    {
     "ename": "InvalidArgumentError",
     "evalue": "`labels` out of bound\nCondition x < y did not hold.\nFirst 3 elements of x:\n[  0 255   0]\nFirst 1 elements of y:\n[25]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "Input \u001b[0;32mIn [38]\u001b[0m, in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m m_seg_path \u001b[38;5;241m=\u001b[39m base \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mm_seg_rural_input_images/\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      2\u001b[0m m_seg_directory \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mlistdir(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mm_seg_rural_input_images/\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 4\u001b[0m iou_with_m_seg \u001b[38;5;241m=\u001b[39m \u001b[43mmodels_to_use\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbase\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mground_truth_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mm_seg_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mground_truth_directory\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mm_seg_directory\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[0;32mIn [37]\u001b[0m, in \u001b[0;36mmodels_to_use\u001b[0;34m(base, ground_truth_path, prediction_model_path, ground_truth_directory, prediction_model_directory)\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(ground_truth_images)):\n\u001b[1;32m     31\u001b[0m     store_images\u001b[38;5;241m.\u001b[39mupdate({ground_truth_images[idx]: prediction_model_images[idx]})\n\u001b[0;32m---> 33\u001b[0m miou_per_set_of_images \u001b[38;5;241m=\u001b[39m \u001b[43mcompare_two_images\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstore_images\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m miou_per_set_of_images\n",
      "Input \u001b[0;32mIn [36]\u001b[0m, in \u001b[0;36mcompare_two_images\u001b[0;34m(stored_images)\u001b[0m\n\u001b[1;32m     19\u001b[0m prediction_model_image \u001b[38;5;241m=\u001b[39m value\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe prediction_model_image is : \u001b[39m\u001b[38;5;124m\"\u001b[39m, prediction_model_image)\n\u001b[0;32m---> 22\u001b[0m i_o_u \u001b[38;5;241m=\u001b[39m \u001b[43mcalc_miou\u001b[49m\u001b[43m(\u001b[49m\u001b[43mground_truth_image\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprediction_model_image\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe Mean IOU for these two images : \u001b[39m\u001b[38;5;124m\"\u001b[39m, i_o_u, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     24\u001b[0m store_iou\u001b[38;5;241m.\u001b[39mappend(i_o_u)\n",
      "Input \u001b[0;32mIn [35]\u001b[0m, in \u001b[0;36mcalc_miou\u001b[0;34m(ground_truth_image, prediction_model_image)\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprediction_model_rgb size:\u001b[39m\u001b[38;5;124m\"\u001b[39m, np\u001b[38;5;241m.\u001b[39mshape(prediction_model_rgb), prediction_model_rgb)\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresized_prediction_model_rgb size:\u001b[39m\u001b[38;5;124m\"\u001b[39m, np\u001b[38;5;241m.\u001b[39mshape(resized_prediction_model_rgb), resized_prediction_model_rgb)\n\u001b[0;32m---> 51\u001b[0m \u001b[43mmean_iou\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate_state\u001b[49m\u001b[43m(\u001b[49m\u001b[43mground_truth_rgb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresized_prediction_model_rgb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m mean_iou\u001b[38;5;241m.\u001b[39mresult()\u001b[38;5;241m.\u001b[39mnumpy()\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/iou/lib/python3.10/site-packages/keras/utils/metrics_utils.py:70\u001b[0m, in \u001b[0;36mupdate_state_wrapper.<locals>.decorated\u001b[0;34m(metric_obj, *args, **kwargs)\u001b[0m\n\u001b[1;32m     64\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m     65\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTrying to run metric.update_state in replica context when \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     66\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mthe metric was not created in TPUStrategy scope. \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     67\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMake sure the keras Metric is created in TPUstrategy scope. \u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     69\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tf_utils\u001b[38;5;241m.\u001b[39mgraph_context_for_symbolic_tensors(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m---> 70\u001b[0m   update_op \u001b[38;5;241m=\u001b[39m \u001b[43mupdate_state_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m update_op \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:  \u001b[38;5;66;03m# update_op will be None in eager execution.\u001b[39;00m\n\u001b[1;32m     72\u001b[0m   metric_obj\u001b[38;5;241m.\u001b[39madd_update(update_op)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/iou/lib/python3.10/site-packages/keras/metrics/base_metric.py:140\u001b[0m, in \u001b[0;36mMetric.__new__.<locals>.update_state_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    137\u001b[0m control_status \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39m__internal__\u001b[38;5;241m.\u001b[39mautograph\u001b[38;5;241m.\u001b[39mcontrol_status_ctx()\n\u001b[1;32m    138\u001b[0m ag_update_state \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39m__internal__\u001b[38;5;241m.\u001b[39mautograph\u001b[38;5;241m.\u001b[39mtf_convert(\n\u001b[1;32m    139\u001b[0m     obj_update_state, control_status)\n\u001b[0;32m--> 140\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mag_update_state\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/iou/lib/python3.10/site-packages/tensorflow/python/autograph/impl/api.py:689\u001b[0m, in \u001b[0;36mconvert.<locals>.decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    687\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    688\u001b[0m   \u001b[38;5;28;01mwith\u001b[39;00m conversion_ctx:\n\u001b[0;32m--> 689\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mconverted_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    690\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# pylint:disable=broad-except\u001b[39;00m\n\u001b[1;32m    691\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(e, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mag_error_metadata\u001b[39m\u001b[38;5;124m'\u001b[39m):\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/iou/lib/python3.10/site-packages/tensorflow/python/autograph/impl/api.py:331\u001b[0m, in \u001b[0;36mconverted_call\u001b[0;34m(f, args, kwargs, caller_fn_scope, options)\u001b[0m\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m conversion\u001b[38;5;241m.\u001b[39mis_in_allowlist_cache(f, options):\n\u001b[1;32m    330\u001b[0m   logging\u001b[38;5;241m.\u001b[39mlog(\u001b[38;5;241m2\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAllowlisted \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m: from cache\u001b[39m\u001b[38;5;124m'\u001b[39m, f)\n\u001b[0;32m--> 331\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_call_unconverted\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    333\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ag_ctx\u001b[38;5;241m.\u001b[39mcontrol_status_ctx()\u001b[38;5;241m.\u001b[39mstatus \u001b[38;5;241m==\u001b[39m ag_ctx\u001b[38;5;241m.\u001b[39mStatus\u001b[38;5;241m.\u001b[39mDISABLED:\n\u001b[1;32m    334\u001b[0m   logging\u001b[38;5;241m.\u001b[39mlog(\u001b[38;5;241m2\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAllowlisted: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m: AutoGraph is disabled in context\u001b[39m\u001b[38;5;124m'\u001b[39m, f)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/iou/lib/python3.10/site-packages/tensorflow/python/autograph/impl/api.py:458\u001b[0m, in \u001b[0;36m_call_unconverted\u001b[0;34m(f, args, kwargs, options, update_cache)\u001b[0m\n\u001b[1;32m    455\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m f\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__self__\u001b[39m\u001b[38;5;241m.\u001b[39mcall(args, kwargs)\n\u001b[1;32m    457\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m kwargs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 458\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    459\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m f(\u001b[38;5;241m*\u001b[39margs)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/iou/lib/python3.10/site-packages/keras/metrics/metrics.py:2494\u001b[0m, in \u001b[0;36m_IoUBase.update_state\u001b[0;34m(self, y_true, y_pred, sample_weight)\u001b[0m\n\u001b[1;32m   2491\u001b[0m     sample_weight \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mreshape(sample_weight, [\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])\n\u001b[1;32m   2493\u001b[0m \u001b[38;5;66;03m# Accumulate the prediction to current confusion matrix.\u001b[39;00m\n\u001b[0;32m-> 2494\u001b[0m current_cm \u001b[38;5;241m=\u001b[39m \u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfusion_matrix\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2495\u001b[0m \u001b[43m    \u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2496\u001b[0m \u001b[43m    \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2497\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_classes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2498\u001b[0m \u001b[43m    \u001b[49m\u001b[43mweights\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2499\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2500\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtotal_cm\u001b[38;5;241m.\u001b[39massign_add(current_cm)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/iou/lib/python3.10/site-packages/tensorflow/python/util/traceback_utils.py:153\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m--> 153\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[1;32m    154\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    155\u001b[0m   \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/iou/lib/python3.10/site-packages/tensorflow/python/ops/check_ops.py:407\u001b[0m, in \u001b[0;36m_binary_assert\u001b[0;34m(sym, opname, op_func, static_func, x, y, data, summarize, message, name)\u001b[0m\n\u001b[1;32m    404\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m message \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    405\u001b[0m     data \u001b[38;5;241m=\u001b[39m [message] \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mlist\u001b[39m(data)\n\u001b[0;32m--> 407\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m errors\u001b[38;5;241m.\u001b[39mInvalidArgumentError(\n\u001b[1;32m    408\u001b[0m       node_def\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    409\u001b[0m       op\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    410\u001b[0m       message\u001b[38;5;241m=\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(_pretty_print(d, summarize) \u001b[38;5;28;01mfor\u001b[39;00m d \u001b[38;5;129;01min\u001b[39;00m data)))\n\u001b[1;32m    412\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:  \u001b[38;5;66;03m# not context.executing_eagerly()\u001b[39;00m\n\u001b[1;32m    413\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m data \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: `labels` out of bound\nCondition x < y did not hold.\nFirst 3 elements of x:\n[  0 255   0]\nFirst 1 elements of y:\n[25]"
     ]
    }
   ],
   "source": [
    "m_seg_path = base + \"m_seg_rural_input_images/\"\n",
    "m_seg_directory = os.listdir(\"m_seg_rural_input_images/\")\n",
    "\n",
    "iou_with_m_seg = models_to_use(base, ground_truth_path, m_seg_path, ground_truth_directory, m_seg_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdded9dc-9549-43c8-9087-436b08672943",
   "metadata": {},
   "outputs": [],
   "source": [
    "pxlib_path = base + \"pxlib_rural_input_images/\"\n",
    "pxlib_directory = os.listdir(\"pxlib_rural_input_images/\")\n",
    "\n",
    "iou_with_pxlib = models_to_use(base, ground_truth_path, pxlib_path, ground_truth_directory, pxlib_directory)\n",
    "# print(iou_with_pxlib)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e06540b5-8864-41b6-8791-138bc685985c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_models_and_iou(ground_truth_directory, iou_with_m_seg, iou_with_pxlib):\n",
    "    '''Take in models with their mean iou values for each image\n",
    "    \n",
    "    ground_truth_directory -- list\n",
    "    iou_with_m_seg -- list\n",
    "    iou_with_pxlib -- list\n",
    "    \n",
    "    Return:\n",
    "    table comparing iou values -- DataFrame\n",
    "    '''\n",
    "    \n",
    "    ground_truth = \"Ground Truth and \"\n",
    "    m_seg_column = ground_truth + \"M Seg\"\n",
    "    pxlib_column = ground_truth + \"PX Lib\"\n",
    "    \n",
    "    data = {m_seg_column: iou_with_m_seg, pxlib_column: iou_with_pxlib}\n",
    "    create_df = pd.DataFrame(data, index=[ground_truth_directory])\n",
    "    \n",
    "    \n",
    "    return create_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7b8ac4d-be7a-4865-bece-613086628675",
   "metadata": {},
   "outputs": [],
   "source": [
    "format_models_and_iou(ground_truth_directory, iou_with_m_seg, iou_with_pxlib)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59c807e3-ec27-4ffa-9e95-959db77186ed",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
